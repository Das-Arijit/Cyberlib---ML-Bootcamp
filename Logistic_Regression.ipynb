{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "uNjDMFkICNSG"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################################## UTILITY FUNCTIONS ######################################################################################"
      ],
      "metadata": {
        "id": "UiFSNFPdOZtq"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' converts from '.csv' file to a numpy array '''\n",
        "def extractData(s):\n",
        "  data_df = pd.read_csv(s, header = None)\n",
        "  return data_df.to_numpy()"
      ],
      "metadata": {
        "id": "uFhHFtX9B9Pa"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' seperates the first column of a np array as index / serial numbers'''\n",
        "def seperateFeatures(data):\n",
        "  return data[:,1:], data[:,0]"
      ],
      "metadata": {
        "id": "M89sgsdrCyCE"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' returns np array with sigmoid value of each entry : sigmoid(x) = 1/(1+exp(-x)) '''\n",
        "def sigmoid (data):\n",
        "  return (1/(1 + np.exp(-data)))"
      ],
      "metadata": {
        "id": "dhFrnuAOGM2f"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' return matrix with an added bias row (of all ones) as the first row of the given matrix '''\n",
        "def addBias(X):\n",
        "  m = X.shape[0]\n",
        "  u = np.ones((m,1))\n",
        "  X = np.append(u, X, axis = 1)\n",
        "  return X"
      ],
      "metadata": {
        "id": "cIMIzx5pOg1I"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' returns np array with values scaled data by the formula : (x-mean)/(standard deviation) '''\n",
        "def meanScaling(data):\n",
        "  mean = np.mean(data, axis=0)\n",
        "\n",
        "  std = np.std(data, axis=0)\n",
        "  for i in range(std.size):\n",
        "    std[i] = 1 if (std[i] == False) else (std[i] == std[i])\n",
        "    \n",
        "  return ((data - np.mean(data, axis=0))/std)"
      ],
      "metadata": {
        "id": "yfG3VvgaatkU"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Calculates accuracy as (number of correct predictions) *100/(total number of predictions)'''\n",
        "def accuracy(h, y):\n",
        "  m = h.size\n",
        "  count = 0\n",
        "  for i in range(m):\n",
        "    if (h[i] == y[i]):\n",
        "      count+=1\n",
        "  return (count*100)/m"
      ],
      "metadata": {
        "id": "5I796CH3zQjs"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################################################################################################################################################"
      ],
      "metadata": {
        "id": "ATx_pGhDOf9x"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''calculates loss for logistic regression'''\n",
        "def logisticCost(y, hypothesis):\n",
        "  m = y.size\n",
        "  cost = -(1/m)*(np.sum(y*np.log(hypothesis)) + np.sum((1-y)*np.log(1-hypothesis)))\n",
        "  return cost"
      ],
      "metadata": {
        "id": "9X_E77QJJWpN"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''calculates  gradient for logistic regression'''\n",
        "def logisticGrad(X, y, hypothesis, reg_coeff, theta):\n",
        "  m = y.shape[0]\n",
        "  k = theta.shape[1]\n",
        "  theta_c = theta.copy()\n",
        "  theta_c[0] = np.zeros((1,k))\n",
        "  grad = (1/m)*(X.T@(hypothesis - y) + reg_coeff*theta_c)\n",
        "  return grad"
      ],
      "metadata": {
        "id": "NZjypowK2sLa"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Converts vector with each entry representing class number to probability matrix with zeros and ones'''\n",
        "def vectorToMatrix(y, num_class):\n",
        "  m = y.size\n",
        "  Y = np.zeros((m, num_class))\n",
        "  count = 0\n",
        "  for i in y:\n",
        "    Y[count][i-1] = 1\n",
        "    count +=1\n",
        "  \n",
        "  return Y"
      ],
      "metadata": {
        "id": "mbfVJDShd3y5"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Converts probability matrix to vector with each entry representing class number'''\n",
        "def matrixToVector(a):\n",
        "  y = np.argmax(a, axis=1)+1\n",
        "  y.shape = (y.size, 1)\n",
        "  return y"
      ],
      "metadata": {
        "id": "wx3p8PEkzNuw"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(X_train, y_train, num_iter, num_class, alpha, reg_coeff):\n",
        "\n",
        "  '''\n",
        "  X_train     --> design matrix (before adding features)\n",
        "  y_train     --> target value / true result (m x 1)\n",
        "  num_iter    --> number of iteration (int)\n",
        "  alpha       --> learning rate\n",
        "  reg_coeff   --> regularisation coefficient\n",
        "  '''\n",
        "  \n",
        "  m, n = X_train.shape\n",
        "  # m --> number of trainig examples\n",
        "  # n --> number of features \n",
        "\n",
        "  #Converting vector y to probability matrix Y\n",
        "  Y = vectorToMatrix(y_train, num_class)\n",
        "\n",
        "  #Scaling and adding bias term to training design matrix\n",
        "  X_train = meanScaling(X_train)\n",
        "  X = addBias(X_train)\n",
        "\n",
        "  #initializing losses\n",
        "  losses = []\n",
        "\n",
        "  #initializing theta\n",
        "  theta = np.zeros((n+1,num_class))\n",
        "\n",
        "  #training loop\n",
        "  for i in range(num_iter):\n",
        "\n",
        "    hypothesis = sigmoid(X@theta)\n",
        "\n",
        "    #calculating and appending losses\n",
        "    losses.append(logisticCost(Y, hypothesis))\n",
        "\n",
        "    #calculating gradient\n",
        "    grad = logisticGrad(X, Y, hypothesis, reg_coeff, theta)\n",
        "\n",
        "    #updating theta\n",
        "    theta -= alpha*grad\n",
        "\n",
        "    print(i, \"loss = \", losses[i])\n",
        "\n",
        "  plt.plot(losses)\n",
        "  return theta"
      ],
      "metadata": {
        "id": "yUTaeuRN6b8n"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, y, theta, num_class):\n",
        "\n",
        "  '''\n",
        "  X           --> Design matrix of features of testing data\n",
        "  y           --> Target value vector of testing data\n",
        "  theta       --> parameter obtained after training\n",
        "  num_classes --> number of classes\n",
        "  '''\n",
        "\n",
        "  m = X.shape[0]\n",
        "  # m = number of testing examples\n",
        "\n",
        "  # scaling and adding bias to test design matrix\n",
        "  X = meanScaling(X)\n",
        "  X = addBias(X)\n",
        "\n",
        "  hypothesis = sigmoid(X@theta)\n",
        "\n",
        "  #converting y vector to probability matrix Y\n",
        "  Y = vectorToMatrix(y, num_class)\n",
        "\n",
        "  #calculating loss\n",
        "  loss = logisticCost(Y, hypothesis)\n",
        "  print(\"loss = \", loss)\n",
        "  \n",
        "  #converting hypothesis (probability matrix) to class vector\n",
        "  y_predicted = matrixToVector(hypothesis)\n",
        "  \n",
        "  print(\"Accuracy = \", accuracy(y_predicted, y))\n",
        "\n",
        "  return y_predicted"
      ],
      "metadata": {
        "id": "3bj-dtHriRiv"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = extractData('/content/drive/MyDrive/WOC/Datasets/emnist-letters-train.csv')\n",
        "X_train, y_train = seperateFeatures(data_train)"
      ],
      "metadata": {
        "id": "Tf4RmVcOte4M"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = extractData('/content/drive/MyDrive/WOC/Datasets/emnist-letters-test.csv')\n",
        "X_test, y_test = seperateFeatures(data_test)"
      ],
      "metadata": {
        "id": "EMzazWWvs3Y0"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theta = train(X_train, y_train, 500, 26, 0.000016, 0)"
      ],
      "metadata": {
        "id": "zSFqcoOQuP2G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e98ea2dc-5c11-449e-de29-2f31aebb8f4f"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 loss =  0.6931471805599486\n",
            "1 loss =  0.6829034011469927\n",
            "2 loss =  0.6804973753503017\n",
            "3 loss =  0.6794307337275202\n",
            "4 loss =  0.6788315131225103\n",
            "5 loss =  0.6784480622536092\n",
            "6 loss =  0.6781785705455955\n",
            "7 loss =  0.6779765865011077\n",
            "8 loss =  0.67781805800376\n",
            "9 loss =  0.6776893577796389\n",
            "10 loss =  0.6775821585887566\n",
            "11 loss =  0.6774910558895361\n",
            "12 loss =  0.6774123706817391\n",
            "13 loss =  0.6773434989833748\n",
            "14 loss =  0.6772825384143915\n",
            "15 loss =  0.6772280618162112\n",
            "16 loss =  0.677178974519724\n",
            "17 loss =  0.6771344208465858\n",
            "18 loss =  0.677093721021237\n",
            "19 loss =  0.6770563273868998\n",
            "20 loss =  0.6770217933044699\n",
            "21 loss =  0.6769897505781943\n",
            "22 loss =  0.676959892764931\n",
            "23 loss =  0.6769319626237733\n",
            "24 loss =  0.6769057425392826\n",
            "25 loss =  0.6768810471161428\n",
            "26 loss =  0.6768577173853274\n",
            "27 loss =  0.676835616222872\n",
            "28 loss =  0.6768146246929746\n",
            "29 loss =  0.6767946391036941\n",
            "30 loss =  0.6767755686178158\n",
            "31 loss =  0.6767573333002254\n",
            "32 loss =  0.6767398625114869\n",
            "33 loss =  0.6767230935781067\n",
            "34 loss =  0.6767069706855002\n",
            "35 loss =  0.6766914439514014\n",
            "36 loss =  0.676676468646373\n",
            "37 loss =  0.6766620045348851\n",
            "38 loss =  0.6766480153157776\n",
            "39 loss =  0.6766344681450371\n",
            "40 loss =  0.67662133322708\n",
            "41 loss =  0.6766085834633038\n",
            "42 loss =  0.6765961941487353\n",
            "43 loss =  0.6765841427092167\n",
            "44 loss =  0.6765724084729142\n",
            "45 loss =  0.676560972470995\n",
            "46 loss =  0.6765498172631815\n",
            "47 loss =  0.6765389267845898\n",
            "48 loss =  0.6765282862108707\n",
            "49 loss =  0.6765178818391043\n",
            "50 loss =  0.6765077009823296\n",
            "51 loss =  0.6764977318758959\n",
            "52 loss =  0.6764879635941001\n",
            "53 loss =  0.6764783859757965\n",
            "54 loss =  0.676468989557864\n",
            "55 loss =  0.6764597655155646\n",
            "56 loss =  0.6764507056089621\n",
            "57 loss =  0.6764418021346903\n",
            "58 loss =  0.6764330478824531\n",
            "59 loss =  0.6764244360957116\n",
            "60 loss =  0.6764159604360949\n",
            "61 loss =  0.6764076149511294\n",
            "62 loss =  0.6763993940449243\n",
            "63 loss =  0.6763912924514953\n",
            "64 loss =  0.6763833052104662\n",
            "65 loss =  0.6763754276448869\n",
            "66 loss =  0.6763676553409694\n",
            "67 loss =  0.6763599841295377\n",
            "68 loss =  0.6763524100690375\n",
            "69 loss =  0.67634492942994\n",
            "70 loss =  0.6763375386804228\n",
            "71 loss =  0.6763302344731943\n",
            "72 loss =  0.6763230136333667\n",
            "73 loss =  0.6763158731472754\n",
            "74 loss =  0.6763088101521625\n",
            "75 loss =  0.676301821926646\n",
            "76 loss =  0.6762949058819041\n",
            "77 loss =  0.6762880595535243\n",
            "78 loss =  0.6762812805939394\n",
            "79 loss =  0.6762745667654194\n",
            "80 loss =  0.6762679159335651\n",
            "81 loss =  0.6762613260612628\n",
            "82 loss =  0.6762547952030621\n",
            "83 loss =  0.6762483214999416\n",
            "84 loss =  0.6762419031744377\n",
            "85 loss =  0.6762355385260971\n",
            "86 loss =  0.6762292259272397\n",
            "87 loss =  0.676222963818994\n",
            "88 loss =  0.6762167507076025\n",
            "89 loss =  0.6762105851609582\n",
            "90 loss =  0.6762044658053662\n",
            "91 loss =  0.6761983913225067\n",
            "92 loss =  0.6761923604465961\n",
            "93 loss =  0.6761863719617184\n",
            "94 loss =  0.6761804246993163\n",
            "95 loss =  0.6761745175358468\n",
            "96 loss =  0.6761686493905659\n",
            "97 loss =  0.6761628192234479\n",
            "98 loss =  0.6761570260332339\n",
            "99 loss =  0.6761512688555857\n",
            "100 loss =  0.6761455467613455\n",
            "101 loss =  0.6761398588549049\n",
            "102 loss =  0.6761342042726541\n",
            "103 loss =  0.6761285821815249\n",
            "104 loss =  0.6761229917776129\n",
            "105 loss =  0.6761174322848726\n",
            "106 loss =  0.6761119029538867\n",
            "107 loss =  0.6761064030607011\n",
            "108 loss =  0.6761009319057166\n",
            "109 loss =  0.6760954888126502\n",
            "110 loss =  0.6760900731275371\n",
            "111 loss =  0.6760846842177911\n",
            "112 loss =  0.6760793214713182\n",
            "113 loss =  0.6760739842956622\n",
            "114 loss =  0.6760686721172062\n",
            "115 loss =  0.676063384380407\n",
            "116 loss =  0.6760581205470647\n",
            "117 loss =  0.6760528800956369\n",
            "118 loss =  0.6760476625205784\n",
            "119 loss =  0.6760424673317135\n",
            "120 loss =  0.6760372940536422\n",
            "121 loss =  0.676032142225172\n",
            "122 loss =  0.6760270113987746\n",
            "123 loss =  0.6760219011400678\n",
            "124 loss =  0.6760168110273276\n",
            "125 loss =  0.676011740651012\n",
            "126 loss =  0.676006689613316\n",
            "127 loss =  0.6760016575277407\n",
            "128 loss =  0.6759966440186801\n",
            "129 loss =  0.6759916487210372\n",
            "130 loss =  0.6759866712798397\n",
            "131 loss =  0.6759817113498863\n",
            "132 loss =  0.6759767685954039\n",
            "133 loss =  0.6759718426897163\n",
            "134 loss =  0.6759669333149295\n",
            "135 loss =  0.6759620401616345\n",
            "136 loss =  0.6759571629286121\n",
            "137 loss =  0.6759523013225582\n",
            "138 loss =  0.6759474550578204\n",
            "139 loss =  0.6759426238561399\n",
            "140 loss =  0.6759378074464083\n",
            "141 loss =  0.6759330055644327\n",
            "142 loss =  0.6759282179527094\n",
            "143 loss =  0.675923444360209\n",
            "144 loss =  0.6759186845421657\n",
            "145 loss =  0.67591393825988\n",
            "146 loss =  0.6759092052805239\n",
            "147 loss =  0.6759044853769607\n",
            "148 loss =  0.675899778327559\n",
            "149 loss =  0.6758950839160284\n",
            "150 loss =  0.6758904019312564\n",
            "151 loss =  0.675885732167141\n",
            "152 loss =  0.6758810744224456\n",
            "153 loss =  0.6758764285006504\n",
            "154 loss =  0.67587179420981\n",
            "155 loss =  0.6758671713624166\n",
            "156 loss =  0.6758625597752691\n",
            "157 loss =  0.6758579592693452\n",
            "158 loss =  0.6758533696696826\n",
            "159 loss =  0.6758487908052556\n",
            "160 loss =  0.6758442225088664\n",
            "161 loss =  0.6758396646170334\n",
            "162 loss =  0.6758351169698809\n",
            "163 loss =  0.6758305794110444\n",
            "164 loss =  0.6758260517875685\n",
            "165 loss =  0.675821533949809\n",
            "166 loss =  0.6758170257513431\n",
            "167 loss =  0.6758125270488796\n",
            "168 loss =  0.6758080377021752\n",
            "169 loss =  0.6758035575739473\n",
            "170 loss =  0.675799086529796\n",
            "171 loss =  0.6757946244381258\n",
            "172 loss =  0.6757901711700692\n",
            "173 loss =  0.6757857265994164\n",
            "174 loss =  0.6757812906025438\n",
            "175 loss =  0.675776863058343\n",
            "176 loss =  0.6757724438481579\n",
            "177 loss =  0.6757680328557208\n",
            "178 loss =  0.6757636299670912\n",
            "179 loss =  0.6757592350705898\n",
            "180 loss =  0.6757548480567487\n",
            "181 loss =  0.675750468818252\n",
            "182 loss =  0.6757460972498782\n",
            "183 loss =  0.6757417332484491\n",
            "184 loss =  0.6757373767127849\n",
            "185 loss =  0.6757330275436422\n",
            "186 loss =  0.6757286856436773\n",
            "187 loss =  0.6757243509173939\n",
            "188 loss =  0.6757200232710979\n",
            "189 loss =  0.6757157026128533\n",
            "190 loss =  0.6757113888524452\n",
            "191 loss =  0.6757070819013268\n",
            "192 loss =  0.675702781672594\n",
            "193 loss =  0.6756984880809311\n",
            "194 loss =  0.6756942010425864\n",
            "195 loss =  0.6756899204753252\n",
            "196 loss =  0.6756856462984022\n",
            "197 loss =  0.6756813784325199\n",
            "198 loss =  0.6756771167997985\n",
            "199 loss =  0.6756728613237448\n",
            "200 loss =  0.6756686119292157\n",
            "201 loss =  0.6756643685423896\n",
            "202 loss =  0.675660131090739\n",
            "203 loss =  0.6756558995029954\n",
            "204 loss =  0.6756516737091282\n",
            "205 loss =  0.6756474536403086\n",
            "206 loss =  0.6756432392288901\n",
            "207 loss =  0.6756390304083785\n",
            "208 loss =  0.6756348271134077\n",
            "209 loss =  0.6756306292797138\n",
            "210 loss =  0.6756264368441124\n",
            "211 loss =  0.6756222497444736\n",
            "212 loss =  0.6756180679197025\n",
            "213 loss =  0.6756138913097108\n",
            "214 loss =  0.6756097198554004\n",
            "215 loss =  0.6756055534986434\n",
            "216 loss =  0.6756013921822539\n",
            "217 loss =  0.6755972358499778\n",
            "218 loss =  0.6755930844464649\n",
            "219 loss =  0.6755889379172566\n",
            "220 loss =  0.6755847962087619\n",
            "221 loss =  0.6755806592682431\n",
            "222 loss =  0.6755765270437969\n",
            "223 loss =  0.6755723994843363\n",
            "224 loss =  0.6755682765395774\n",
            "225 loss =  0.675564158160018\n",
            "226 loss =  0.6755600442969245\n",
            "227 loss =  0.6755559349023185\n",
            "228 loss =  0.6755518299289572\n",
            "229 loss =  0.675547729330323\n",
            "230 loss =  0.6755436330606037\n",
            "231 loss =  0.6755395410746861\n",
            "232 loss =  0.675535453328137\n",
            "233 loss =  0.675531369777187\n",
            "234 loss =  0.6755272903787265\n",
            "235 loss =  0.6755232150902845\n",
            "236 loss =  0.6755191438700224\n",
            "237 loss =  0.6755150766767162\n",
            "238 loss =  0.6755110134697495\n",
            "239 loss =  0.6755069542090985\n",
            "240 loss =  0.675502898855323\n",
            "241 loss =  0.675498847369554\n",
            "242 loss =  0.6754947997134835\n",
            "243 loss =  0.675490755849355\n",
            "244 loss =  0.6754867157399503\n",
            "245 loss =  0.6754826793485846\n",
            "246 loss =  0.6754786466390884\n",
            "247 loss =  0.6754746175758051\n",
            "248 loss =  0.6754705921235821\n",
            "249 loss =  0.6754665702477556\n",
            "250 loss =  0.6754625519141452\n",
            "251 loss =  0.675458537089047\n",
            "252 loss =  0.6754545257392214\n",
            "253 loss =  0.6754505178318844\n",
            "254 loss =  0.6754465133347065\n",
            "255 loss =  0.6754425122157943\n",
            "256 loss =  0.6754385144436911\n",
            "257 loss =  0.675434519987363\n",
            "258 loss =  0.6754305288161976\n",
            "259 loss =  0.6754265408999914\n",
            "260 loss =  0.6754225562089444\n",
            "261 loss =  0.6754185747136543\n",
            "262 loss =  0.6754145963851093\n",
            "263 loss =  0.6754106211946783\n",
            "264 loss =  0.6754066491141084\n",
            "265 loss =  0.6754026801155167\n",
            "266 loss =  0.6753987141713844\n",
            "267 loss =  0.6753947512545491\n",
            "268 loss =  0.6753907913382006\n",
            "269 loss =  0.675386834395874\n",
            "270 loss =  0.6753828804014442\n",
            "271 loss =  0.6753789293291205\n",
            "272 loss =  0.675374981153441\n",
            "273 loss =  0.6753710358492652\n",
            "274 loss =  0.675367093391773\n",
            "275 loss =  0.6753631537564548\n",
            "276 loss =  0.6753592169191097\n",
            "277 loss =  0.6753552828558369\n",
            "278 loss =  0.675351351543034\n",
            "279 loss =  0.6753474229573933\n",
            "280 loss =  0.6753434970758928\n",
            "281 loss =  0.6753395738757947\n",
            "282 loss =  0.6753356533346377\n",
            "283 loss =  0.6753317354302375\n",
            "284 loss =  0.6753278201406797\n",
            "285 loss =  0.6753239074443117\n",
            "286 loss =  0.6753199973197472\n",
            "287 loss =  0.675316089745853\n",
            "288 loss =  0.6753121847017518\n",
            "289 loss =  0.6753082821668149\n",
            "290 loss =  0.6753043821206581\n",
            "291 loss =  0.67530048454314\n",
            "292 loss =  0.6752965894143561\n",
            "293 loss =  0.6752926967146363\n",
            "294 loss =  0.675288806424541\n",
            "295 loss =  0.6752849185248578\n",
            "296 loss =  0.6752810329965977\n",
            "297 loss =  0.6752771498209925\n",
            "298 loss =  0.6752732689794895\n",
            "299 loss =  0.6752693904537508\n",
            "300 loss =  0.6752655142256476\n",
            "301 loss =  0.6752616402772597\n",
            "302 loss =  0.6752577685908713\n",
            "303 loss =  0.6752538991489648\n",
            "304 loss =  0.6752500319342237\n",
            "305 loss =  0.6752461669295253\n",
            "306 loss =  0.6752423041179384\n",
            "307 loss =  0.6752384434827242\n",
            "308 loss =  0.6752345850073269\n",
            "309 loss =  0.6752307286753777\n",
            "310 loss =  0.6752268744706869\n",
            "311 loss =  0.675223022377245\n",
            "312 loss =  0.6752191723792167\n",
            "313 loss =  0.6752153244609426\n",
            "314 loss =  0.6752114786069323\n",
            "315 loss =  0.6752076348018656\n",
            "316 loss =  0.6752037930305881\n",
            "317 loss =  0.675199953278108\n",
            "318 loss =  0.6751961155295957\n",
            "319 loss =  0.6751922797703824\n",
            "320 loss =  0.6751884459859542\n",
            "321 loss =  0.6751846141619533\n",
            "322 loss =  0.6751807842841743\n",
            "323 loss =  0.6751769563385617\n",
            "324 loss =  0.6751731303112093\n",
            "325 loss =  0.6751693061883572\n",
            "326 loss =  0.6751654839563878\n",
            "327 loss =  0.6751616636018302\n",
            "328 loss =  0.6751578451113495\n",
            "329 loss =  0.6751540284717509\n",
            "330 loss =  0.6751502136699784\n",
            "331 loss =  0.6751464006931079\n",
            "332 loss =  0.6751425895283492\n",
            "333 loss =  0.6751387801630433\n",
            "334 loss =  0.6751349725846613\n",
            "335 loss =  0.6751311667808005\n",
            "336 loss =  0.6751273627391849\n",
            "337 loss =  0.6751235604476641\n",
            "338 loss =  0.6751197598942071\n",
            "339 loss =  0.6751159610669084\n",
            "340 loss =  0.6751121639539774\n",
            "341 loss =  0.6751083685437448\n",
            "342 loss =  0.6751045748246546\n",
            "343 loss =  0.6751007827852693\n",
            "344 loss =  0.6750969924142611\n",
            "345 loss =  0.6750932037004155\n",
            "346 loss =  0.6750894166326289\n",
            "347 loss =  0.6750856311999056\n",
            "348 loss =  0.6750818473913589\n",
            "349 loss =  0.6750780651962064\n",
            "350 loss =  0.6750742846037727\n",
            "351 loss =  0.6750705056034818\n",
            "352 loss =  0.6750667281848668\n",
            "353 loss =  0.6750629523375549\n",
            "354 loss =  0.6750591780512748\n",
            "355 loss =  0.675055405315856\n",
            "356 loss =  0.6750516341212229\n",
            "357 loss =  0.6750478644573962\n",
            "358 loss =  0.6750440963144909\n",
            "359 loss =  0.6750403296827159\n",
            "360 loss =  0.6750365645523742\n",
            "361 loss =  0.6750328009138562\n",
            "362 loss =  0.6750290387576439\n",
            "363 loss =  0.6750252780743117\n",
            "364 loss =  0.6750215188545164\n",
            "365 loss =  0.675017761089005\n",
            "366 loss =  0.6750140047686091\n",
            "367 loss =  0.6750102498842473\n",
            "368 loss =  0.6750064964269162\n",
            "369 loss =  0.6750027443877024\n",
            "370 loss =  0.674998993757768\n",
            "371 loss =  0.6749952445283586\n",
            "372 loss =  0.6749914966908012\n",
            "373 loss =  0.6749877502364974\n",
            "374 loss =  0.674984005156929\n",
            "375 loss =  0.674980261443655\n",
            "376 loss =  0.6749765190883101\n",
            "377 loss =  0.674972778082603\n",
            "378 loss =  0.6749690384183187\n",
            "379 loss =  0.674965300087313\n",
            "380 loss =  0.6749615630815156\n",
            "381 loss =  0.6749578273929285\n",
            "382 loss =  0.6749540930136225\n",
            "383 loss =  0.6749503599357405\n",
            "384 loss =  0.6749466281514924\n",
            "385 loss =  0.6749428976531585\n",
            "386 loss =  0.6749391684330854\n",
            "387 loss =  0.6749354404836871\n",
            "388 loss =  0.6749317137974435\n",
            "389 loss =  0.6749279883668986\n",
            "390 loss =  0.6749242641846631\n",
            "391 loss =  0.6749205412434096\n",
            "392 loss =  0.6749168195358742\n",
            "393 loss =  0.6749130990548566\n",
            "394 loss =  0.6749093797932174\n",
            "395 loss =  0.6749056617438763\n",
            "396 loss =  0.6749019448998167\n",
            "397 loss =  0.6748982292540785\n",
            "398 loss =  0.6748945147997628\n",
            "399 loss =  0.6748908015300275\n",
            "400 loss =  0.6748870894380881\n",
            "401 loss =  0.674883378517219\n",
            "402 loss =  0.6748796687607489\n",
            "403 loss =  0.6748759601620623\n",
            "404 loss =  0.6748722527145999\n",
            "405 loss =  0.6748685464118571\n",
            "406 loss =  0.6748648412473823\n",
            "407 loss =  0.6748611372147774\n",
            "408 loss =  0.6748574343076972\n",
            "409 loss =  0.6748537325198479\n",
            "410 loss =  0.6748500318449893\n",
            "411 loss =  0.6748463322769311\n",
            "412 loss =  0.6748426338095334\n",
            "413 loss =  0.6748389364367057\n",
            "414 loss =  0.674835240152408\n",
            "415 loss =  0.6748315449506493\n",
            "416 loss =  0.6748278508254854\n",
            "417 loss =  0.674824157771022\n",
            "418 loss =  0.6748204657814117\n",
            "419 loss =  0.6748167748508529\n",
            "420 loss =  0.6748130849735916\n",
            "421 loss =  0.6748093961439191\n",
            "422 loss =  0.6748057083561702\n",
            "423 loss =  0.6748020216047295\n",
            "424 loss =  0.6747983358840212\n",
            "425 loss =  0.6747946511885169\n",
            "426 loss =  0.6747909675127282\n",
            "427 loss =  0.6747872848512139\n",
            "428 loss =  0.674783603198573\n",
            "429 loss =  0.6747799225494474\n",
            "430 loss =  0.6747762428985189\n",
            "431 loss =  0.6747725642405142\n",
            "432 loss =  0.6747688865701978\n",
            "433 loss =  0.6747652098823768\n",
            "434 loss =  0.6747615341718977\n",
            "435 loss =  0.6747578594336453\n",
            "436 loss =  0.6747541856625465\n",
            "437 loss =  0.6747505128535645\n",
            "438 loss =  0.6747468410017012\n",
            "439 loss =  0.6747431701019994\n",
            "440 loss =  0.6747395001495368\n",
            "441 loss =  0.6747358311394285\n",
            "442 loss =  0.6747321630668284\n",
            "443 loss =  0.6747284959269262\n",
            "444 loss =  0.6747248297149471\n",
            "445 loss =  0.674721164426153\n",
            "446 loss =  0.6747175000558409\n",
            "447 loss =  0.6747138365993435\n",
            "448 loss =  0.6747101740520273\n",
            "449 loss =  0.6747065124092955\n",
            "450 loss =  0.6747028516665824\n",
            "451 loss =  0.6746991918193589\n",
            "452 loss =  0.6746955328631274\n",
            "453 loss =  0.6746918747934251\n",
            "454 loss =  0.6746882176058205\n",
            "455 loss =  0.6746845612959154\n",
            "456 loss =  0.6746809058593453\n",
            "457 loss =  0.6746772512917745\n",
            "458 loss =  0.6746735975889018\n",
            "459 loss =  0.6746699447464537\n",
            "460 loss =  0.6746662927601925\n",
            "461 loss =  0.6746626416259087\n",
            "462 loss =  0.674658991339421\n",
            "463 loss =  0.674655341896583\n",
            "464 loss =  0.6746516932932748\n",
            "465 loss =  0.6746480455254064\n",
            "466 loss =  0.6746443985889183\n",
            "467 loss =  0.674640752479779\n",
            "468 loss =  0.6746371071939851\n",
            "469 loss =  0.6746334627275635\n",
            "470 loss =  0.6746298190765697\n",
            "471 loss =  0.6746261762370834\n",
            "472 loss =  0.6746225342052155\n",
            "473 loss =  0.6746188929771034\n",
            "474 loss =  0.6746152525489115\n",
            "475 loss =  0.6746116129168311\n",
            "476 loss =  0.6746079740770791\n",
            "477 loss =  0.6746043360259021\n",
            "478 loss =  0.6746006987595693\n",
            "479 loss =  0.6745970622743765\n",
            "480 loss =  0.6745934265666484\n",
            "481 loss =  0.6745897916327295\n",
            "482 loss =  0.674586157468995\n",
            "483 loss =  0.6745825240718423\n",
            "484 loss =  0.6745788914376945\n",
            "485 loss =  0.6745752595629975\n",
            "486 loss =  0.6745716284442245\n",
            "487 loss =  0.6745679980778713\n",
            "488 loss =  0.6745643684604564\n",
            "489 loss =  0.6745607395885247\n",
            "490 loss =  0.6745571114586412\n",
            "491 loss =  0.6745534840673971\n",
            "492 loss =  0.6745498574114053\n",
            "493 loss =  0.6745462314873029\n",
            "494 loss =  0.674542606291747\n",
            "495 loss =  0.6745389818214191\n",
            "496 loss =  0.6745353580730237\n",
            "497 loss =  0.6745317350432849\n",
            "498 loss =  0.67452811272895\n",
            "499 loss =  0.6745244911267894\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "h = predict(X_train, y_train, theta, 26)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INKPP6DPtoLK",
        "outputId": "f355c043-41d1-46ab-b237-5b2bc222afed"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss =  0.6745208702335936\n",
            "Accuracy =  57.792792792792795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "h = predict(X_test, y_test, theta, 26)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33jOt3BZ9p-8",
        "outputId": "e76fca9f-9ce2-4e29-b056-8ffc2493ed10"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss =  0.676593966461495\n",
            "Accuracy =  55.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ejGbRCoN9_Nz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}